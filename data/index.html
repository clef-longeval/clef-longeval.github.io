<!DOCTYPE html>
<html lang='en'>

<head>
    <base href=".">
    <link rel="shortcut icon" type="image/png" href="https://clef-longeval.github.io/assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="https://clef-longeval.github.io/assets/main.css"/>
    <meta name="description" content="Conference Template">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>LongEval 2023</title>
</head>

<body>

    <div class="banner">
        <img src="https://clef-longeval.github.io/assets/banner.jpg" alt="Conference Template Banner">
        <div class="top-left">
            <span class="title1">LongEval CLEF 2023 Lab</span>
        </div>
        <div class="bottom-right">
            Longitudinal Evaluation of Model Performance
        </div>
    </div>

    <table class="navigation">
        <tr>
            <td class="navigation">
                <a class="current" title="description" href="https://clef-longeval.github.io/">Description</a>
            </td>
            <td class="navigation">
                <a title="Dates" href="https://clef-longeval.github.io/dates">Dates</a>
            </td>
            <td class="navigation">
                <a title="Organizers" href="https://clef-longeval.github.io/organizers">Organizers</a> 
            </td>
            <td class="navigation">
                <a title="Tasks" href="https://clef-longeval.github.io/tasks">Tasks</a>
            </td>
            <td class="navigation">
                <a title="Data" href="https://clef-longeval.github.io/data">Data</a>
            </td>
            <td class="navigation">
                <a title="Submissions" href="https://clef-longeval.github.io/submissions">Submissions</a>
            </td>
        </tr>
    </table>

    <h1>Data</h1>
    <h2>  Task 1. LongEval-Retrieval </h2>     
    <p>The data for this task is a sequence of web document collections and queries provided by Qwant.</p>
    <b>Data</b>
    <p>Queries:<br>The queries are extracted from Qwant’s search logs, based on a set of selected topics. The initial set of extracted queries are filtered to exclude spam and queries returning a small number of documents. The query set was created in French and was automatically translated to English.</p>
    <p>Documents:<br>The document collection first includes relevant documents that are selected to be retrieved for each query. The first step for creating the document collection is to extract from the index the content of all the documents that have been displayed in SERPs for the queries that we selected. In addition to these documents, potentially non-relevant documents are randomly sampled from Qwant index in order to better represent the nature of a Web test collection. A random sampling process has been applied to alleviate bias and prevalence of relevant documents. Filters have also been applied to exclude spam and adult content.</p>
    <p>Relevance estimates:<br>The relevance estimates for LongEval-Retrieval are obtained through automatic collection of user implicit feedback. This implicit feedback is obtained with a click model, based on Dynamic Bayesian Networks trained on Qwant data. The output of the click model represents an attractiveness probability, which is turned to a 3-level scale score ( 0 = not relevant, 1 = relevant, 2 = highly relevant). This set of relevance estimates will be completed with explicit relevance assessment after the submission deadline. </p>
    <p>The overview of the data creation process is displayed in the Figure below:</p>
    <p><img src="https://clef-longeval.github.io/assets/collection-process.png"></p>
    <br>
    <b>Train / Test collections (within a time train + heldout / short term / long term)</b>
    <p>The training data and the heldout within a time query set can be downloaded from  <a href="http://hdl.handle.net/11234/1-5010">Lindat/Clarin website</a>. If you experience any problems with loggin to the Lindat/Clarin website, please first check <a href="https://lindat.mff.cuni.cz/en/how-do-i-sign-up">the instructions</a> and <a href="mailto:longeval-ir-task@univ-grenoble-alpes.fr">contact</a> the organizers. You can find the Readme with the details of the collection <a href="https://docs.google.com/document/d/1Ozng4hXsvOKoF0j3w037i09WpIXfLMzZp1incsVhUP8/edit?usp=sharing">here</a>.</p>
    <p>Data in this collection was acquired during June 2022. The document corpus consist of 1,570,734 Web pages. The queries in this Train collection were randomly split into train and heldout queries. The collection consists of 672 train queries, with corresponding 9,656 assessments and 98 heldout queries with corresponding 1,420 assessments. There are thus in average 14 assessments per query. About 73% of the assessments are non-relevant (7,030 assessments on the train queries in total), 21% are relevant (2,028 assessments) and 6% are highly relevant (598 assessments). The table below shows example queries:</p>
    <p>
    <table>
     <tr>
       <th><b>Query ID</b></th>
       <th><b>French Query</b></th>
      <th><b>English Query</b></th>
     </tr>
     <tr>
       <td>q06229550</td>
       <td>bareme impots</td>
       <td>Taxation</td>
     </tr>
     <tr>
       <td>q06223863</td>
       <td>consommation eau</td>
       <td>consumption water</td>
     </tr>
     <tr>
       <td>q06221247</td>
       <td>gateau aux pommes</td>
       <td>apple cake</td>
     </tr>
      <tr>
       <td>q06225303</td>
       <td>offre emploi</td>
       <td>offer of employment</td>
     </tr>  
     </table> 
    </p>
    <p>While we publish the assessments for the train queries, we keep the assessments for the heldout queries hidden from the participants and only release them after publishing the official task results. </p>
    <p>We further plan to release two test collections. The short-term persistence collection was collected over July 2022 and the long-term persistence collection over September 2022. The collections will be according to the shared task timeline released in April 2023. The participants are required to submit the results for either one or both test collections and also to submit the results on the heldout Train queries described above. </p>
    <br>
    <b>References:</b><br> 
    P. Galuscakova, R. Deveaud, G. Gonzalez-Saez, P. Mulhem, L. Goeuriot, F. Piroi, M. Popel: <a href="https://arxiv.org/abs/2303.03229">LongEval-Retrieval: French-English Dynamic Test Collection for Continuous Web Search Evaluation</a>. 
    <br>
    <h2> Task 2. LongEval-Classification </h2>
    <p>
<!--         For model training, we plan to use the <a href="https://figshare.com/articles/dataset/TM-Senti/16438281">TM-Senti dataset</a> 
        and extend it with a novel test set for evaluation of participants's' submissions. TM-Senti is a general large-scale tweets 
        sentiment dataset in the English language spanning a 9-year period ranging from 2013 to 2021. Tweets are labelled for sentiment
        as either “positive” or “negative”. The annotation is performed using distant supervision based on a manually curated list of
        emojis and <a href="https://arxiv.org/abs/2108.13898?context=cs.CL">emoticons</a> and, thus, can be easily extended to 
        cover more recent years. <br> <br> -->
        
        <b> <a href="https://drive.google.com/file/d/1aWItzobrcw-DR4ZalMoJXDcL7yluOpGm/view?usp=sharing"> Training data with two temporal practice sets </a>  </b> <br>
        <b> <a href="https://github.com/Rababalkhalifa/LongEval2023/blob/main/format_checker.py"> Format checking script </a> </b> <br>
        <b> <a href="http://tweetnlp.org/downloads/sample-10k-monthly-120k-yearly.jl.zip"> Unlablled data </a> </b> <br>  
        <b> <a href="https://github.com/Rababalkhalifa/LongEval2023/blob/main/baseline_results.7z"> Baseline model results </a>  </b> <br>
        <b> <a href="https://github.com/Rababalkhalifa/LongEval2023/blob/9bf97bee07a1bbee195a05767f4b886aa69dbcf5/Baseline_Script_CLEF_LongEval_Classification_2022_Task_2_Subtask_B_1_2_%5BBaseline%5D.ipynb"> Example - Baseline model script </a> </b> <br>   
        <b> Evaluation script - TBA  </b> <br> 

</body>
</html>
