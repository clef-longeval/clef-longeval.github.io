<!DOCTYPE html>
<html lang='en'>

<head>
    <base href=".">
    <link rel="shortcut icon" type="image/png" href="https://clef-longeval.github.io/assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="https://clef-longeval.github.io/assets/main.css"/>
    <meta name="description" content="Conference Template">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>LongEval 2024</title>
</head>

<body>

    <div class="banner">
        <img src="https://clef-longeval.github.io/assets/banner.jpg" alt="Conference Template Banner">
        <div class="top-left">
            <span class="title1">LongEval CLEF 2024 Lab</span>
        </div>
        <div class="bottom-right">
            Longitudinal Evaluation of Model Performance
        </div>
    </div>

    <table class="navigation">
        <tr>
            <td class="navigation">
                <a class="Description" title="description" href="https://clef-longeval.github.io/">Description</a>
            </td>
            <td class="navigation">
                <a title="Dates" href="https://clef-longeval.github.io/dates">Dates</a>
            </td>
            <td class="navigation">
                <a title="Organizers" href="https://clef-longeval.github.io/organizers">Organizers</a> 
            </td>
            <td class="navigation">
                <a title="Tasks" href="https://clef-longeval.github.io/tasks">Tasks</a>
            </td>
            <td class="navigation">
                <a title="current" href="https://clef-longeval.github.io/data">Data</a>
            </td>
            <td class="navigation">
                <a title="Submissions" href="https://clef-longeval.github.io/submissions">Submissions</a>
            </td>
            <td class="navigation">
                <a title="2023" href="https://clef-longeval-2023.github.io/"> 2023</a>
            </td>
        </tr>
    </table>

    <h1>Data</h1>
    <b><h2>  Task 1. LongEval-Retrieval </h2> </b>    
    <p>The data for this task is a sequence of web document collections and queries provided by Qwant.</p>
    <b>Data</b>
    <p>Queries:<br>The queries are extracted from Qwant’s search logs, based on a set of selected topics. The initial set of extracted queries are filtered to exclude spam and queries returning a small number of documents. The query set was created in French and was automatically translated to English.</p>
    <p>Documents:<br>The document collection first includes relevant documents that are selected to be retrieved for each query. The first step for creating the document collection is to extract from the index the content of all the documents that have been displayed in SERPs for the queries that we selected. In addition to these documents, potentially non-relevant documents are randomly sampled from Qwant index in order to better represent the nature of a Web test collection. A random sampling process has been applied to alleviate bias and prevalence of relevant documents. Filters have also been applied to exclude spam and adult content.</p>
    <p>Relevance estimates:<br>The relevance estimates for LongEval-Retrieval are obtained through automatic collection of user implicit feedback. This implicit feedback is obtained with a click model, based on Dynamic Bayesian Networks trained on Qwant data. The output of the click model represents an attractiveness probability, which is turned to a 3-level scale score (0 = not relevant, 1 = relevant, 2 = highly relevant). This set of relevance estimates will be completed with explicit relevance assessment after the submission deadline. </p>
    <p>The overview of the data creation process is displayed in the Figure below:</p>
    <p><img src="https://clef-longeval.github.io/assets/collection-process.png"></p>
    <br>
    <b>Train collections</b>
    <p>The training data can be downloaded from Lindat/Clarin website: <a href="http://hdl.handle.net/11234/1-5010">June, 2023 train set</a> and <a href="http://hdl.handle.net/11234/1-5139">July and September, 2023 test sets</a>. The relevance judgements from the click model for July and September data can also be downloaded from the <a href="http://hdl.handle.net/11234/1-5151">Lindat/Clarin website</a>. If you experience any problems with loggin to the Lindat/Clarin website, please first check <a href="https://lindat.mff.cuni.cz/en/how-do-i-sign-up">the instructions</a> and <a href="mailto:longeval-ir-task@univ-grenoble-alpes.fr">contact</a> the organizers. You can find the Readme with the details of the train collection <a href="https://docs.google.com/document/d/1Ozng4hXsvOKoF0j3w037i09WpIXfLMzZp1incsVhUP8/edit?usp=sharing">here regarding June data</a> and <a href="https://docs.google.com/document/d/1xbW52276exB1jDkp90tqqcdHyR-hS2Hd_Tz_Ctnjz8I/edit?usp=sharing">here regarding July and September's data</a></p>
    <p>June 2022 dataset consist of 1,570,734 Web pages. The queries in this June's collection were randomly split into train and heldout queries. The collection consists of 672 train queries, with corresponding 9,656 assessments and 98 heldout queries with corresponding 1,420 assessments. There are thus in average 14 assessments per query. About 73% of the assessments are non-relevant (7,030 assessments on the train queries in total), 21% are relevant (2,028 assessments) and 6% are highly relevant (598 assessments). 
    July 2022 dataset contains 1,593,376 documents and 882 queries. This collection was used as a test collection for short-term persistence sub-task. The data for the long-term persistence sub-task was collected over September 2022 and this dataset consists of 1,081,334 documents and 923 queries. Information about the collections can also be <a href="https://github.com/clef-longeval/IR-Participants/blob/8def52e2f4844fa53f3f1fac3695c6cc65dd8e1b/Data/Readme.md">downloaded here</a>
    The table below shows example queries:</p>
    <p>
    <table>
     <tr>
       <th><b>Query ID</b></th>
       <th><b>French Query</b></th>
      <th><b>English Query</b></th>
     </tr>
     <tr>
       <td>q06229550</td>
       <td>bareme impots</td>
       <td>Taxation</td>
     </tr>
     <tr>
       <td>q06223863</td>
       <td>consommation eau</td>
       <td>consumption water</td>
     </tr>
     <tr>
       <td>q06221247</td>
       <td>gateau aux pommes</td>
       <td>apple cake</td>
     </tr>
      <tr>
       <td>q06225303</td>
       <td>offre emploi</td>
       <td>offer of employment</td>
     </tr>  
     </table> 
    </p>

    <br>
    <b>References:</b><br> 
    P. Galuscakova, R. Deveaud, G. Gonzalez-Saez, P. Mulhem, L. Goeuriot, F. Piroi, M. Popel: <a href="https://arxiv.org/abs/2303.03229">LongEval-Retrieval: French-English Dynamic Test Collection for Continuous Web Search Evaluation</a>. 
    <br>
    <b><h2> Task 2. LongEval-Classification </h2></b>
    <p>
<!--         For model training, we plan to use the <a href="https://figshare.com/articles/dataset/TM-Senti/16438281">TM-Senti dataset</a> 
        and extend it with a novel test set for evaluation of participants's' submissions. TM-Senti is a general large-scale tweets 
        sentiment dataset in the English language spanning a 9-year period ranging from 2013 to 2021. Tweets are labelled for sentiment
        as either “positive” or “negative”. The annotation is performed using distant supervision based on a manually curated list of
        emojis and <a href="https://arxiv.org/abs/2108.13898?context=cs.CL">emoticons</a> and, thus, can be easily extended to 
        cover more recent years. <br> <br> -->
        
    
        <b> <h3>Practice [Pre-Evaluation]</h3> </b>
        <i> <b>You can access the <a href="https://codalab.lisn.upsaclay.fr/competitions/12762">COMPETITION HERE </a> and submit to <u>Practice</u> to evaluate your model and practice submittion process  </i> </b> <br> 
        <b> You can download the training and practice sets from here: <a href="https://drive.google.com/file/d/1aWItzobrcw-DR4ZalMoJXDcL7yluOpGm/view?usp=sharing"> Training data with two temporal practice sets </a>  </b> <br>
        <br><i><u><b> CodaLab Submission Format </b></u></i> <br> 
        When submitting to Codalab, please submit a single zip file containing a folder called “submission”. This folder must contain THREE files:  <br> 
        1. predicted_eval_within.txt (with within predictions - interim_eval_2016.json)  <br> 
        2. predicted_eval_short.txt  (with distant predictions - interim_eval_2018.json)  <br> 
        3. predicted_eval_long.txt   (a BLANK file which will be used for interim_eval_2021.json during the evaluation phase)
        
        <br>
        <b> <h3>Evaluation</h3> </b> 
        <i> <b>You can access the <a href="https://codalab.lisn.upsaclay.fr/competitions/12762"> COMPETITION HERE </a> and submit to <u>Evaluation</u> to evaluate your model and rank its performance  </i> </b> <br> 
        <b> You can download the evaluation set from here: <a href="https://drive.google.com/file/d/1y_M91wI2mK5UrRx0nsyz_Y94jVsBjten/view?usp=sharing"> Three temporal evaluation sets without gold labels </a>  </b> <br>
        <b> Evaluation Golden Labeles released: <a href="https://github.com/Rababalkhalifa/LongEval2023/blob/main/evaluation-gold.zip"> Three temporal evaluation sets with gold labels </a>  </b> <br>
        <br><i><u><b> CodaLab Submission Format </b></u></i> <br> 
        When submitting to Codalab, please submit a single zip file containing a folder called “submission”. This folder must contain THREE files:  <br> 
        1. predicted_test_within.txt (with within predictions - interim_test_2016.json)  <br> 
        2. predicted_test_short.txt  (with distant/short predictions - interim_test_2018.json)  <br> 
        3. predicted_test_long.txt   (with distant/long predictions - interim_test_2021.json)  <br> 
        
        <b> <h3>Notes</h3> </b> 
        <h5>  Use <a href="https://github.com/Rababalkhalifa/LongEval2023/blob/main/format_checker.py"> Format checking script </a> </b> for test your formatting and look into examples provided here: <a href="https://github.com/Rababalkhalifa/LongEval2023/blob/main/baseline_results.7z"> Baseline model results </a>  <h5>
        
        <h4><b>Starting-kit:<b></h4>
        <b> Temporal data: <a href="http://tweetnlp.org/downloads/sample-10k-monthly-120k-yearly.jl.zip"> raw unlablled data </a> </b> <br>  
        <b> Example for reading temporal data: <a href="https://colab.research.google.com/drive/1gOwCqtRRNcYsNozRYeF_2pAl8VAoGulv?usp=sharing"> unlablled data </a> </b> <br>
        <b> Example of submittion: <a href="https://github.com/Rababalkhalifa/LongEval2023/blob/main/baseline_results.7z"> model results </a>  </b> <br>
       
        <br><h3> <b>  Good Luck! </b> </h3> <br> 
 <br> 
</body>
</html>
