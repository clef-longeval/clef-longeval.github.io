<!DOCTYPE html>
<html lang='en'>

<head>
    <base href=".">
    <link rel="shortcut icon" type="image/png" href="https://clef-longeval.github.io/assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="https://clef-longeval.github.io/assets/main.css"/>
    <meta name="description" content="Conference Template">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>LongEval 2024</title>
</head>

<body>

    <div class="banner">
        <img src="https://clef-longeval.github.io/assets/banner.jpg" alt="Conference Template Banner">
        <div class="top-left">
            <span class="title1">LongEval CLEF 2024 Lab</span>
        </div>
        <div class="bottom-right">
            Longitudinal Evaluation of Model Performance
        </div>
    </div>

    <table class="navigation">
        <tr>
            <td class="navigation">
                <a title="description" href="https://clef-longeval.github.io/">Description</a>
            </td>
            <td class="navigation">
                <a title="Dates" href="https://clef-longeval.github.io/dates">Dates</a>
            </td>
            <td class="navigation">
                <a title="Organizers" href="https://clef-longeval.github.io/organizers">Organizers</a> 
            </td>
            <td class="navigation">
                <a title="Tasks" href="https://clef-longeval.github.io/tasks">Tasks</a>
            </td>
            <td class="navigation">
                <a class="current" title="Data" href="https://clef-longeval.github.io/data">Data</a>
            </td>
            <td class="navigation">
                <a title="Submissions" href="https://clef-longeval.github.io/submissions">Submissions</a>
            </td>
            <td class="navigation">
                <a title="2023" href="https://clef-longeval-2023.github.io/"> 2023</a>
            </td>
        </tr>
    </table>

    <h1>Data</h1>
    <b><h2>  Task 1. LongEval-Retrieval </h2> </b>    
    <p>The data for this task is a sequence of web document collections and queries provided by Qwant.</p>
    <b>Description of the Data</b>
    <p><b>Queries:</b><br>The queries are extracted from Qwant’s search logs, based on a set of selected topics. The query set was created in French and was automatically translated to English. To deal with the translation quality, 4 translations are provided for each query, sorted by their estimated translation probability.</p>
    <p><b>Documents:</b><br>The document collection includes documents that are selected to be retrieved for each query. The first step for creating the document collection is to extract from the index the content of all the documents that have been displayed in SERPs for the queries that we selected. In addition to these documents, potentially non-relevant documents are randomly sampled from Qwant index in order to better represent the nature of a Web test collection. A random sampling process has been applied to alleviate bias and prevalence of relevant documents. Filters were applied to exclude spam and adult content.</p>
    <p><b>Relevance estimates:</b><br>The relevance estimates for LongEval-Retrieval are obtained through automatic collection of user implicit feedback. This implicit feedback is obtained with a click model, based on Dynamic Bayesian Networks trained on Qwant data. The output of the click model represents an attractiveness probability, which is turned to a 3-level scale score (0 = not relevant, 1 = relevant, 2 = highly relevant). This set of relevance estimates will be completed with explicit relevance assessment after the submission deadline. </p>
    <p>The overview of the data creation process is displayed in the Figure below:</p>
    <p><img src="https://clef-longeval.github.io/assets/collection-process.png"></p>
    <br>
    <b>Collections</b>
	<p>Participants to LongEval 2024 can use the LongEval 2023 for training as well.<br>
	    <ul>
			<li> - <b>2023 Training set:</b> composed of documents collected in <a href="http://hdl.handle.net/11234/1-5010">June 2022, queries, and qrels</a> (on the Lindat/Clarin website).</li>
			<li> - <b>2023 Test set:</b> composed of documents collected in <a href="http://hdl.handle.net/11234/1-5139">July and September 2022, queries</a>, and, separately, <a href="http://hdl.handle.net/11234/1-5151">the qrels</a> (both datasets on the Lindat/Clarin website).</li>
			<li> - <b>2023 Training set:</b> composed of documents collected in <a href="https://doi.org/10.48436/y60e9-k9b51">January 2023, queries, and qrels</a> (on the TU Wien Research Data Repository).</li>
			<li>- <b>2024 Test Set:</b> composed of documents collected in <a href="https://researchdata.tuwien.at/records/wm79f-88x06">June and August 2023, queries, and qrels</a> (on the TU Wien Research Data Repository). IMPORTANT: As if April 11, 2024, the test data contains only the French documents. The English translation is still running, as soon as it is ready we will publish it on this page.</li>
    </ul>
	
    <p> If you experience any problems with loggin to the Lindat/Clarin website, please first check <a href="https://lindat.mff.cuni.cz/en/how-do-i-sign-up">the instructions</a> and <a href="mailto:longeval-ir-task@univ-grenoble-alpes.fr">contact</a> the organizers. </p>
    <br>
    <b>References:</b><br> 
    <p>More details about the collection can be found in a paper:
    P. Galuscakova, R. Deveaud, G. Gonzalez-Saez, P. Mulhem, L. Goeuriot, F. Piroi, M. Popel: <a href="https://arxiv.org/abs/2303.03229">LongEval-Retrieval: French-English Dynamic Test Collection for Continuous Web Search Evaluation</a>. 
    </p>
    <b><h2> Task 2. LongEval-Classification </h2></b>
    <p>
<!--         For model training, we plan to use the <a href="https://figshare.com/articles/dataset/TM-Senti/16438281">TM-Senti dataset</a> 
        and extend it with a novel test set for evaluation of participants's' submissions. TM-Senti is a general large-scale tweets 
        sentiment dataset in the English language spanning a 9-year period ranging from 2013 to 2021. Tweets are labelled for sentiment
        as either “positive” or “negative”. The annotation is performed using distant supervision based on a manually curated list of
        emojis and <a href="https://arxiv.org/abs/2108.13898?context=cs.CL">emoticons</a> and, thus, can be easily extended to 
        cover more recent years. <br> <br> -->
        
    
        <b> <h3>Practice [Pre-Evaluation]</h3> </b>
        <i> <b>You can access the <a href="https://codalab.lisn.upsaclay.fr/competitions/12762">COMPETITION HERE </a> and submit to <u>Practice</u> to evaluate your model and practice submittion process  </i> </b> <br> 
        <b> You can download the training and practice sets from here: <a href="https://drive.google.com/file/d/1aWItzobrcw-DR4ZalMoJXDcL7yluOpGm/view?usp=sharing"> Training data with two temporal practice sets </a>  </b> <br>
        <br><i><u><b> CodaLab Submission Format </b></u></i> <br> 
        When submitting to Codalab, please submit a single zip file containing a folder called “submission”. This folder must contain THREE files:  <br> 
        1. predicted_eval_within.txt (with within predictions - interim_eval_2016.json)  <br> 
        2. predicted_eval_short.txt  (with distant predictions - interim_eval_2018.json)  <br> 
        3. predicted_eval_long.txt   (a BLANK file which will be used for interim_eval_2021.json during the evaluation phase)
        
        <br>
        <b> <h3>Evaluation</h3> </b> 
        <i> <b>You can access the <a href="https://codalab.lisn.upsaclay.fr/competitions/12762"> COMPETITION HERE </a> and submit to <u>Evaluation</u> to evaluate your model and rank its performance  </i> </b> <br> 
        <b> You can download the evaluation set from here: <a href="https://drive.google.com/file/d/1y_M91wI2mK5UrRx0nsyz_Y94jVsBjten/view?usp=sharing"> Three temporal evaluation sets without gold labels </a>  </b> <br>
        <b> Evaluation Golden Labeles released: <a href="https://github.com/Rababalkhalifa/LongEval2023/blob/main/evaluation-gold.zip"> Three temporal evaluation sets with gold labels </a>  </b> <br>
        <br><i><u><b> CodaLab Submission Format </b></u></i> <br> 
        When submitting to Codalab, please submit a single zip file containing a folder called “submission”. This folder must contain THREE files:  <br> 
        1. predicted_test_within.txt (with within predictions - interim_test_2016.json)  <br> 
        2. predicted_test_short.txt  (with distant/short predictions - interim_test_2018.json)  <br> 
        3. predicted_test_long.txt   (with distant/long predictions - interim_test_2021.json)  <br> 
        
        <b> <h3>Notes</h3> </b> 
        <h5>  Use <a href="https://github.com/Rababalkhalifa/LongEval2023/blob/main/format_checker.py"> Format checking script </a> </b> for test your formatting and look into examples provided here: <a href="https://github.com/Rababalkhalifa/LongEval2023/blob/main/baseline_results.7z"> Baseline model results </a>  <h5>
        
        <h4><b>Starting-kit:<b></h4>
        <b> Temporal data: <a href="http://tweetnlp.org/downloads/sample-10k-monthly-120k-yearly.jl.zip"> raw unlablled data </a> </b> <br>  
        <b> Example for reading temporal data: <a href="https://colab.research.google.com/drive/1gOwCqtRRNcYsNozRYeF_2pAl8VAoGulv?usp=sharing"> unlablled data </a> </b> <br>
        <b> Example of submittion: <a href="https://github.com/Rababalkhalifa/LongEval2023/blob/main/baseline_results.7z"> model results </a>  </b> <br>
       
        <br><h3> <b>  Good Luck! </b> </h3> <br> 
 <br> 
</body>
</html>
