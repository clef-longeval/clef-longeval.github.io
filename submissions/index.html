<!DOCTYPE html>
<html lang='en'>

<head>
    <base href=".">
    <link rel="shortcut icon" type="image/png" href="https://clef-longeval.github.io/assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="https://clef-longeval.github.io/assets/main.css"/>
    <meta name="description" content="Conference Template">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>LongEval 2024</title>
</head>

<body>

    <div class="banner">
        <img src="https://clef-longeval.github.io/assets/banner.jpg" alt="Conference Template Banner">
        <div class="top-left">
            <span class="title1">LongEval CLEF 2024 Lab</span>
        </div>
        <div class="bottom-right">
            Longitudinal Evaluation of Model Performance
        </div>
    </div>

    <table class="navigation">
        <tr>
            <td class="navigation">
                <a class="current" title="description" href="https://clef-longeval.github.io/">Description</a>
            </td>
            <td class="navigation">
                <a title="Dates" href="https://clef-longeval.github.io/dates">Dates</a>
            </td>
            <td class="navigation">
                <a title="Organizers" href="https://clef-longeval.github.io/organizers">Organizers</a> 
            </td>
            <td class="navigation">
                <a title="Tasks" href="https://clef-longeval.github.io/tasks">Tasks</a>
            </td>
            <td class="navigation">
                <a title="Data" href="https://clef-longeval.github.io/data">Data</a>
            </td>
            <td class="navigation">
                <a title="Submissions" href="https://clef-longeval.github.io/submissions">Submissions</a>
            </td>
            <td class="navigation">
                <a title="2023" href="https://clef-longeval-2023.github.io/"> 2023</a>
            </td>
        </tr>
    </table>

    <h1>Submissions</h1>
    <h2> Task 1. LongEval-Retrieval: </h2>
    <p>Link: <a href="https://github.com/clef-longeval/IR-Participants.git">https://github.com/clef-longeval/IR-Participants.git</a></p>
    <p>Submissions will be done using git. Each team will have a private repository created by the organizers, which will be used to submit the runs to the task. The information about the repository is for each team provided by the organizers. Please contact the organizers if you did not receive this information. 
    <br><br>
    All the participants can either participate in Sub-task A, short-term persistence, Sub-task B, long-term persistence, or both these sub-tasks. Participants can submit up to 5 systems applied to either one or both tasks. For each system submitted on either sub-task, the participants also need to submit the results acquired by this system on Train heldout queries which represent within a time benchmark estimate. This allows the organizers to acquire the information about the change of the system’s performance. Participants also need to provide a short description of each of the submitted systems.
    <br><br>
    We further denote the submission of a single system on a single sub-task or on the within a time query set as the <b>run</b>. The individual runs need to be submitted in the TREC format. For each query in each run, it is allowed to return up to 1000 documents.
    </p>
    <p>
    Each system should be submitted in a single zipped file consisting of a following tuple:<br>
     <ul>
    <li>team_system.WT</li>
    <li>team_system.ST</li>
    <li>team_system.LT</li>
    <li>team_system.meta</li>
    </ul>
    </p>
    <p>
    <b>team_system.WT</b> contains a run (a single TREC file) of the system on the Train heldout queries, used to measure within a time (WT) performance.<br>
    <b>team_system.ST</b> contains a run (a single TREC file) of the system acquired on the Short Term Test queries.<br>
    <b>team_system.LT</b> contains a run (a single TREC file) of the system acquired on the Long Term Test queries. <br>
    Please note, that if the participants decide to only participate in the short-term or only in the long-term sub-task, then team_system.LT or team_system.ST respectively might be missing. However, the participants in all cases need to include team_system.WT and team_system.description files.<br>
    <b>team_system.meta</b> contains a short description of the approach. This file should contain the information which indexing and ranking methods were applied, what type of training was applied and which training data were used. Please specify if you used statistical or neural approaches and if you used sparse or dense retrieval methods. Also, please include the information if the approach uses a single ranking approach, multiple-stages of rankers or any (and what) other type of fusion. Participants also need to describe if they used French data, provided English translations or their own translations and the resources (memory, GPUs, CPUs) used. Participants should use <a href="https://github.com/clef-longeval/IR-Participants/blob/9709a89dc59b093e9a6fd99a29666ae05ebf1c9e/Submissions/team_system.meta">this provided form</a> for filling all the system details.<br>
    <p>All the files in a single zipped document should thus correspond to a single system. The name of this system should contain the name of the team and an unique identifier of this system. The suffix of each file should either be WT, ST, LT or meta. For example, if the file contains the submission of the BM25 system of the UGA team applied on the long-term task, the file name can be UGA_BM25.LT. </p>

    <p>Each system might be either run on French or English data (or their combination). The participants might also opt to use their own translations systems or even manual translations. 
    However, if <b>any manual intervention</b> is used, even for the translation, participants need to clearly state this in the system description.</p>
    
    <h2> Task 2. LongEval-Classification: </h2>
    
<!--     <p> Participants are expected to propose temporally persistent classifiers based on state-of-the-art data-centric or architecture-centric computational methods. 
    The goal is to achieve high weighted-F1 performance across short and long temporally distant test sets while maintaining a reasonable RPD when compared 
    to a test set from the same time period as training. We intend to use <a href="https://huggingface.co/roberta-base">RoBERTa</a> as a baseline classifier
    for our task because it has been demonstrated to be persistent over time. </p>
    
    <p> When the data is released in January 2023, all necessary resources, including python-based baseline code, evaluation scripts, and unlabeled temporal data, will be made available to participating teams. </p>
     -->
    <b> <h3>Practice [Pre-Evaluation]</h3> </b>
        <i> <b>You can access the <a href="https://codalab.lisn.upsaclay.fr/competitions/12762">COMPETITION HERE </a> and submit to <u>Practice</u> to evaluate your model and practice submittion process  </i> </b> <br> 
        <b> You can download the training and practice sets from here: <a href="https://drive.google.com/file/d/1aWItzobrcw-DR4ZalMoJXDcL7yluOpGm/view?usp=sharing"> Training data with two temporal practice sets </a>  </b> <br>
        <br><i><u><b> Submission format </b></u></i> <br> 
        When submitting to Codalab, please submit a single zip file containing a folder called “submission”. This folder must contain THREE files:  <br> 
        1. predicted_eval_within.txt (with within predictions - interim_eval_2016.json)  <br> 
        2. predicted_eval_short.txt  (with distant predictions - interim_eval_2018.json)  <br> 
        3. predicted_eval_long.txt   (a BLANK file which will be used for interim_eval_2021.json during the evaluation phase)
        
        <br>
        <b> <h3>Evaluation</h3> </b> 
        <i> <b>You can access the <a href="https://codalab.lisn.upsaclay.fr/competitions/12762"> COMPETITION HERE </a> and submit to <u>Evaluation</u> to evaluate your model and rank its performance  </i> </b> <br> 
        <b> You can download the evaluation set from here: <a href="https://drive.google.com/file/d/1y_M91wI2mK5UrRx0nsyz_Y94jVsBjten/view?usp=sharing"> Three temporal evaluation sets without gold labels </a>  </b> <br>
        <br><i><u><b> Submission format </b></u></i> <br> 
        When submitting to Codalab, please submit a single zip file containing a folder called “submission”. This folder must contain THREE files:  <br> 
        1. predicted_test_within.txt (with within predictions - interim_test_2016.json)  <br> 
        2. predicted_test_short.txt  (with distant/short predictions - interim_test_2018.json)  <br> 
        3. predicted_test_long.txt   (with distant/long predictions - interim_test_2021.json)  <br> 
        
         
     
    <b> <h3>Notes</h3> </b>
    <p><h5>  Use <a href="https://github.com/Rababalkhalifa/LongEval2023/blob/main/format_checker.py"> Format checking script </a> </b> for test your formatting and look into examples provided here: <a href="https://github.com/Rababalkhalifa/LongEval2023/blob/main/baseline_results.7z"> Baseline model results </a>  </h5></p>
    <p><h5> The submissions for each sub-task will be ranked based on the first metric of macro-averaged F1. We encourage participants to contribute 
        to both sub-tasks in order to be correctly placed on a
        joint leader board, as well as to enable better analysis of their system performance in both settings. </h5></p>

</body>
</html>
