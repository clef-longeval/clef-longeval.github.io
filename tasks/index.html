<!DOCTYPE html>
<html lang='en'>

<head>
    <base href=".">
    <link rel="shortcut icon" type="image/png" href="https://clef-longeval.github.io/assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="https://clef-longeval.github.io/assets/main.css"/>
    <meta name="description" content="Conference Template">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>LongEval 2025</title>
</head>

<body>

    <div class="banner">
        <img src="https://clef-longeval.github.io/assets/banner.jpg" alt="Conference Template Banner">
        <div class="top-left">
            <span class="title1">LongEval CLEF 2025 Lab</span>
        </div>
        <div class="bottom-right">
            Longitudinal Evaluation of Model Performance
        </div>
    </div>


    <table class="navigation">
        <tr>
            <td class="navigation">
                <a title="description" href="https://clef-longeval.github.io/">Description</a>
            </td>
            <td class="navigation">
                <a title="Dates" href="https://clef-longeval.github.io/dates">Dates</a>
            </td>
            <td class="navigation">
                <a title="Organizers" href="https://clef-longeval.github.io/organizers">Organizers</a> 
            </td>
            <td class="navigation">
                <a class="current" title="Tasks" href="https://clef-longeval.github.io/tasks">Tasks</a>
            </td>
            <td class="navigation">
                <a  title="Data" href="https://clef-longeval.github.io/data">Data</a>
            </td>
            <td class="navigation">
                <a  title="Submissions" href="https://clef-longeval.github.io/submissions">Submissions</a>
            </td>
            <td class="navigation">
                <a title="2024" href="https://clef-longeval.github.io/2024">2024</a>
            </td>
            <td class="navigation">
                <a title="2023" href="https://clef-longeval-2023.github.io/"> 2023</a>
            </td>
        </tr>
    </table>
	

    <h1>Tasks</h1>

    <p> In this third iteration the LongEval Lab at CLEF continues to explore
        the temporal dynamics in IR. This includes the potential and limitations of
        temporal relevance signals for ranking, temporal robustness of systems, and novel
        evaluation methods factoring time. By that, the lab sensitizes for the temporal
        uncertain validity of conventional evaluations in IR. Considering the temporal
        dimension provides a new perspective on search and ultimately leads to a more
        holistic view on the retrieval problem.
    </p>
    <p> This year, the lab provides a unique test bed comprising <b>two evolving test
            collections</b>.
        They cover the established retrieval scenarios of Web search and
        scientific retrieval, which have different goals and distinct dynamics.
        Participants are invited to submit retrieval runs to two tasks that cope with these dynamics.
    </p>


    <h2> Task 1. LongEval-WebRetrieval: </h2>

    <b>Objectives:</b> <br>

    <p>The goal of Task 1 is to propose an information retrieval system which can handle
        changes over the time.

        The proposed retrieval system should follow the temporal timewise evolution of Web documents.
        In LongEval, we propose to use evolving Web data to evaluate IR systems longitudinally:
        the systems are expected to be persistent in their retrieval effectiveness over
        time. The systems are evaluated on monthly several snapshots of documents
        and queries (lags), derived from real data acquired from a French Web search
        engine, Qwant.

    </p>


    <p>The collection aims at answering fundamental questions on the robustness and the
        stability of Web search engines against the evolution of the data. Regarding
        Websearch evaluation, LongEval focuses on the following questions:
    </p>
    <p>
    <ul>
        <li>
            (1) How does a search engine behave as the collection of documents available to it evolves? Such a question
            is especially important for commercial systems, as the satisfaction of users is central to such systems.
        </li>
        <li>
            (2) When do we need to update an IR system as the collection of documents to be searched in changes? If we
            are able to assess the decrease in performance (if any) of a system on an evolving collection, we may then
            decide if the system needs to be updated.
        </li>
    </ul>

    <p>
        In comparison to LongEval 2023 and 2024, in this iteration, we enlarge the training and test
        collections with additional snapshots that will allow fine-grained analysis of
        changes in the data collection from one snapshot to another.
    </p>

    <p>To assess an information retrieval system, we provide several datasets of a changing Web documents and users’
        queries:</p>
    <ul>
        <li>
            - <b>Training set:</b> 12 monthly data snapshots, acquired from <b> June 2022 to February 2023 </b> composed
            of documents (18M), queries (9k) , and qrels.
        </li>
        <li> - <b>Test set:</b>, 7 monthly data snapshots, acquired from <b>March 2023 to August 2023</b>, composed of
            documents and queries. </li>
    </ul>

    In total, we will release <b>19 datasets</b> considering documents and queries adressed at the specific month
    snapshot.

    <h2> Task 2. LongEval-SciRetrieval: </h2>


    <p>
        The second task of the LongEval 2025 Lab is similar to the first task, and aims to
        examine how IR systems’ effectiveness changes over time, when the underlying
        document collection changes, where the documents are scientific publications.
        The documents that will make the dataset for this task are acquired from the
        <a title="3 CORE (COnnecting REpositories)" href="https://core.ac.uk/"> CORE </a> 
        collection of scholarly documents. To our knowledge, CORE is currently
        the largest aggregated collection of Open Access full text scholarly documents.
        CORE provides a range of services built on top of this content and these
        services are currently used by over 30 million unique users each month. CORE
        Search provides a web UI for users to query the entire database of scholarly
        documents. This service registers over one million searches each month.
    </p>

    <p> Similarly to Task 1, we will use the click information to drive the 
        relevance assessments for the test collections which consists of two main components 
        that contain both the search and click information : 
    </p>

    <ul>
        <li>
            - <b> Search Information</b> includes i) unique (anonymous) identifiers
            for individual user session; ii) search query; iii) returned results.
        </li>
        <li> - <b>Click Information</b> records, for each click, i) a unique (anonymous)
            identifier for individual user session; ii) the link that was clicked in
            the results list; iii) the position of clicked link in results list. </li>
    </ul>

    <p>Since this is the first time this task is organized, the number of dataset lags is
        lower than those used in the first task. We aim to release two training datasets
        and one or two test datasets.</p>


    <h1>Evaluation</h1>
    <p> The submitted systems will be evaluated in two ways: </p>
    <p>(1) <b><i>nDCG scores</i></b> calculated on provided test sets. Such a classical evaluation measure is consistent
        with Web search, for which the discount emphasises the ordering of the top results. </p>
    <p>(2) <b><i>Relative nDCG Drop (RnD)</i></b> measured by computing the difference between Lag5 and Lag7 test sets.
        This measure supports the evaluation of the impact of the data changes on the systems’ results. </p>
    <p>These measures will be used to assess the extent to which systems provide good results, but also the extent to
        which they are robust against the changes within the data (queries/documents) along time. Using these evaluation
        measures, a system that has good results using nDCG, and also good results according to the RnD measure is
        considered to be able to cope with the evolution over time of the Information Retrieval collection.</p>

</body>

</html>