<!DOCTYPE html>
<html lang='en'>

<head>
    <base href=".">
    <link rel="shortcut icon" type="image/png" href="https://clef-longeval.github.io/assets/favicon.png" />
    <link rel="stylesheet" type="text/css" media="all" href="https://clef-longeval.github.io/assets/main.css" />
    <meta name="description" content="Conference Template">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>LongEval 2026</title>
</head>

<body>

    <div data-include="/includes/banner.html" id="site-banner"></div>

    <div data-include="/includes/nav.html" id="site-nav"></div>


    <h3>Tasks</h3>

    <p>
        LongEval 2026 continues the Sci-Retrieval task from 2025 and introduces three new tasks: LongEval-TopEx
        (Topic Extraction), LongEval-USim (User Simulation), and LongEval-RAG (Retrieval-Augmented Generation).
        Short descriptions, data sources and evaluation procedures for each task appear below. To receive
        announcements and participate in discussions, please subscribe to our <a href="https://groups.google.com/g/longeval">Google Group</a>
        or join the project Slack at <a href="https://longeval.slack.com">longeval.slack.com</a>.
    </p>

    <div class="task-card">
        <h4 class="task-card-title">Task 1. LongEval-Sci: Ad-Hoc Scientific Retrieval</h4>
        <div class="task-hint">
        <div class="task-section">
            <h5 class="task-section-title">Description</h5>
            <p>Build IR systems that keep retrieval effectiveness over time as the scientific collection evolves. Submit runs on multiple time-stamped snapshots.</p>
        </div>
        <div class="task-section">
            <h5 class="task-section-title">Data</h5>
            <p>CORE documents and queries across multiple snapshots. Two training snapshots (e.g. 2024-11, 2025-01) and two test snapshots for short- and long-term persistence (~4M docs, ~1k queries).</p>
        </div>
        <div class="task-section">
            <h5 class="task-section-title">Evaluation</h5>
            <p>nDCG per snapshot; robustness across snapshots measured by Relative Improvement (RI), Delta Relative Improvement (DRI) and Effect Ratio (ER).</p>
        </div>
        </div>
    </div>

    <div class="task-card">
        <h4 class="task-card-title">Task 2.  LongEval-TopEx: Topic Extraction From Query Logs</h4>
        <div class="task-hint">
        <div class="task-section">
            <h5 class="task-section-title">Description</h5>
            <p>Extract TREC-style topics (query + description + narrative) from query logs to formalize the information need aligned with observed usage.</p>
        </div>
        <div class="task-section">
            <h5 class="task-section-title">Data</h5>
            <p>Training snapshot at time t; build a top-10 pool from Task-1 runs overlapping training queries and annotate with multiple LLM-based relevance assessors to obtain alternative qrels per topic.</p>
        </div>
        <div class="task-section">
            <h5 class="task-section-title">Evaluation</h5>
            <p>Assess topics on Alignment (agreement with click-log qrels), Distinguishability (ability to separate runs via nDCG) and Clarity (consistency across LLM annotations). Measure short- and long-term drops. Topics feed Task-3.</p>
        </div>
        </div>
    </div>

    <div class="task-card">
        <h4 class="task-card-title">Task 3. LongEval-USim: User Simulation</h4>
        <div class="task-hint">
        <div class="task-section">
            <h5 class="task-section-title">Task Description</h5>
            <p>A central goal of the LongEval lab is to measure the performance of retrieval models over time. 
                Other models, like user models, were not in focus. Like retrieval models, user models are not 
                meant to be static and time-agnostic, although in practice, time and the evolution of retrieval 
                environments are often ignored. Therefore, in Task 3, we aim to evaluate user models in 
                longitudinal user simulations. </p>
            <p>The core assignment in Subtask 3 is <strong>next query prediction</strong>. 
                This task was previously introduced 
                in the <a href="https://sim4ia.org/sigir2025/#micro-shared-task">Sim4IA micro shared task</a>. 
                For LongEval, we focus on the longitudinal aspects of this user simulation task. The objective 
                is to accurately predict the final query of a given session (which the organizers will withhold), 
                based on the preceding interaction history and the best-fitting user simulator.</p>
            <p>For each provided session, participants must submit up to 5 candidate queries, ranked by their 
                likelihood of being the next query.</p> 
        </div>
        <div class="task-section">
            <h5 class="task-section-title">Data & setup</h5>
            <p>Participants will receive the sessions containing the following rich set of features:</p>
            <ul>
                <li>The sequence of queries submitted.</li>
                <li>Timestamps for query submissions.</li>
                <li>The top-10 SERP (Search Engine Results Page) retrieved for each query.</li>
                <li>Which documents were clicked, along with their corresponding timestamps.</li>
            </ul>
            <p>The data will be provided in the following form:</p>
            <pre>
Index, User Name, Session Number, Query String, Timestamp of Query Submission, 
SERP, SearchID, "[('DocId', Time of Click, Type of Click)]"</pre>
            <p>Session File Example:</p>
            <pre>
1,test_user,1,cultural female genital mutilation,2025-01-10 20:55:48,"[49700987, 41108296, 70340641, 8872149, 151014331, 68561928, 122589791, 126475067, 70353000, 163079162]",4c17b92490816aea96cbcc112f222913,"[('122589791', datetime.datetime(2025, 1, 10, 20, 56, 10), 'works')]"
2,test_user,1,cultural female genital mutilation egypt,2025-01-10 20:57:03,"[107753778, 134385557, 134967534, 31315918, 15106563, 46676819, 45507658, 34140770, 62051567, 46753109]",de73be5d8bf71a6bc877615ebb29b447,"[('15106563', datetime.datetime(2025, 1, 10, 20, 57, 26), 'works')]"
3,test_user,1, ...</pre>
        </div>
        <div class="task-section">
            <h5 class="task-section-title">Task</h5>
            <p>Produce simulated queries or user-model configurations that replicate observed session behavior; train on historic logs and evaluate on held-out sessions.</p>
        </div>
        <div class="task-section">
            <h5 class="task-section-title">Evaluation</h5>
            <p>Compare simulated vs real queries/interactions: indistinguishability and performance-prediction. Use reproducibility measures and track short-/long-term simulator quality.</p>
        </div>
        </div>
    </div>

    <div class="task-card">
        <h4 class="task-card-title">Task 4. LongEval-RAG: Retrieval Augmented Generation (RAG)</h4>
        <div class="task-hint">
        <div class="task-section">
            <h5 class="task-section-title">Description & data</h5>
            <p>The Task 4 aims at studying to which extent does RAGs cope with  evolution of scientific knowledge in time.  Do to that, we provide the task participants a dataset composed  
            of two parts:<br>
            i) A textual query (along with its query id) for which the  participating system has to provide textual answer; <br>  
            ii) A set of document ids, mined from the corpus, on which the answer generation for particular query  must be solelyÂ  based (this set may be seen as a first stage filtering in a RAG system). This set of document is not expected to be composed  only of relevant documents to the query, but it also contains some relevant documents. The provided documents will be a part of the dataset provided within the Task 1. </p> 
            
            <p>The problem that has to be solved by the participants of this task is to provide (based on the query and the list of provided documents):<br>
            a) The generated answer using the extracts of documents in part b) below. <br>
            b) the document extracts that are relevant to the query (text extracted from the textual content of the documents provided in ii), using the contents from the LongEval 2026 corpus).</p>
        </div>
        <div class="task-section">
            <h5 class="task-section-title">Submission</h5>
            <p>For the submission of participants runs:<br>
            The responses for the part a) will be submitted as a string corresponding to the generated answer: (query_id, answer) <br>
            The responses for the part b) will be submitted as a set of pairs (maximum 5): <br>   
            (query_id, {document extract}) : where a document extract is a character string considered to be relevant by the participant for the query. As several extracts may be defined, a run will contain a set of extracts.<br> 
            The examples of an exact submission format and 3 training queries (based on the 2025 dataset) will be provided by the end of February.</p>  
            <p>Overall, each submitted run is a zip file composed of  three text files, one for the a) for all queries, one for b) for all queries, and one file "description.txt" that shortly describes the submitted system. The name of the zip file will be used as the run name during the evaluation process.<br>  
            A maximum of 3 runs may be submitted by each participating team.</p>
            <p>Example of task 4 query (based on the 2025 dataset):<br>  
            - Query_id: 00005<br>
            - Query: "What operation dominates the cost of homomorphic AES evaluation, and how can hardware architectures reduce its impact?" <br>
            - Docs_id: {7964767, 66637364, 42856664, 31316619, 19943691, 19531678,156069243, 156069094, 150704088, 141753453} <br>
            </p>
            <p>The result for this query:<br>
            - Part a: (query_id: 00005, answer: "Large-degree polynomial multiplication dominates the cost of homomorphic AES evaluation. Hardware architectures reduce its impact by decomposing coefficients using CRT, performing fast NTT-based multiplications in parallel, and minimizing modular conversion overhead through optimized arithmetic pipelines.")  <br>
            - Part b: (query_id: 00005, extracts: {"Polynomial multiplication dominates AES evaluation cost."", "Hardware mitigation via CRT + NTT +  optimized modular arithmetic."})
            </p>
        </div>
        <div class="task-section">
            <h5 class="task-section-title">Evaluation</h5>
            <p> The evaluation of a) will be done using classical text generation evaluation measure, such as ROUGE or BERTscore. The evaluation of b) will be done using the computation of the overlap (e.g. Lenvenshtein distance) between the manually assessed  relevant extracts and the retrieved parts.  </p>
        </div>
        </div>
    </div>

    <script src="/assets/js/include.js"></script>
</body>

</html>
