<!DOCTYPE html>
<html lang='en'>

<head>
    <base href=".">
    <link rel="shortcut icon" type="image/png" href="https://clef-longeval.github.io/assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="https://clef-longeval.github.io/assets/main.css"/>
    <meta name="description" content="Conference Template">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>LongEval 2024</title>
</head>

<body>

    <div class="banner">
        <img src="https://clef-longeval.github.io/assets/banner.jpg" alt="Conference Template Banner">
        <div class="top-left">
            <span class="title1">LongEval CLEF 2024 Lab</span>
        </div>
        <div class="bottom-right">
            Longitudinal Evaluation of Model Performance
        </div>
    </div>

    <table class="navigation">
        <tr>
            <td class="navigation">
                <a title="description" href="https://clef-longeval.github.io/">Description</a>
            </td>
            <td class="navigation">
                <a title="Dates" href="https://clef-longeval.github.io/dates">Dates</a>
            </td>
            <td class="navigation">
                <a title="Organizers" href="https://clef-longeval.github.io/organizers">Organizers</a> 
            </td>
            <td class="navigation">
                <a class="current" title="Tasks" href="https://clef-longeval.github.io/tasks">Tasks</a>
            </td>
            <td class="navigation">
                <a title="Data" href="https://clef-longeval.github.io/data">Data</a>
            </td>
            <td class="navigation">
                <a title="Submissions" href="https://clef-longeval.github.io/submissions">Submissions</a>
            </td>
            <td class="navigation">
                <a title="2023" href="https://clef-longeval-2023.github.io/"> 2023</a>
            </td>
        </tr>
    </table>

    <h1>Tasks</h1>
    <p> In this edition of the LongEval Lab, we look at the temporal persistence of the systems’ performance. In order to include the feature of temporal persistence as an additional quality for models proposed, participants are asked to suggest temporal IR systems (Task 1) and longitudinal text classifiers (Task 2) that generalize well beyond a train set generated within a limited time frame. </p>
    <p> We consider two types of temporal persistence tasks: temporal information retrieval and longitudinal text classification. For each task, we look at a short-term and a long-term performance persistence. We aim to answer a high level question: </p>
    <p> <i><b> Given a longitudinal evolving benchmark for a typical NLP task, what types of models offer better temporal persistence over a short term and a long term? </b></i> </p>
    

    <h2> Task 1. LongEval-Retrieval: </h2>
    
    <b>Objectives:</b> <br>
    <p>The goal of Task 1 is to propose an information retrieval system which can handle changes over the time. The proposed retrieval system should follow the temporal timewise evolution of Web documents. The Longeval Websearch collection relies on a large set of data (corpus of pages, queries, user interaction) provided by a commercial search engine (Qwant). It is designed to reflect the changes of the Web across time, by providing evolving document and query sets. The queries in the collection were collected from Qwant's users over several months and can thus be expected to reflect the changes in the search preferences of the users. The documents in the collection were then selected to be able to well evaluate retrieval on these queries at the time they were collected, and thus also change over a time.</p>
    <p>The collection aims at answering fundamental questions on the robustness and the stability of Web search engines against the evolution of the data. Regarding Websearch evaluation, LongEval focuses on the following questions:</p>
    <p>
    <ul>
    <li>
    (1) How does a search engine behave as the collection of documents available to it evolves? Such a question is especially important for commercial systems, as the satisfaction of users is central to such systems.
    </li>
    <li>
    (2) When do we need to update an IR system as the collection of documents to be searched in changes ? If we are able to assess the decrease in performance (if any) of a system on an evolving collection, we may then decide if the system needs to be updated.
    </li>
    </ul>
    <p>To assess an information retrieval system, we provide several datasets which are 3 snapshots of a changing Web documents and users’ queries:</p>
    <ul>
    <li>
        - <b>Training set</b>, acquired at time <i>t1 (June 2022), t2 (July 2022) and t3 (September 2022)</i>  composed of documents, queries, and qrels. These sets were used in the task in year 2023.
    </li>
    <li>
    - One heldout <b>within a time query set</b>, acquired at time <i>t1</i> (without assessments), dedicated to evaluate the quality of the information retrieval system on the corpus of documents acquired at t1. The output of running an IR system on this data will be used as a reference for the two sub-tasks (see below). This test set will be used to assess the initial performance of the trained models.
    </li>    
    <li>- One <b>test set for sub-task A</b>, acquired at time t4, composed of documents and queries. </li>
    <li>- One <b>test set for sub-task B</b>, acquired at time t5, composed of documents and queries.</li>
    </ul>

    <b>Subtasks</b><br>
    <p><b>Sub-task A, short-term persistence.</b> In this task, participants will be asked to examine the retrieval effectiveness when the test documents are dated right after the documents available in the train collection.</p>
    <p><b>Sub-task B, long-term persistence.</b> In this task, participants will be asked to examine retrieval effectiveness on the documents published 3 months after the documents in the train collection were published.</p>
    
    <h2>Task 2. LongEval-Classification: </h2> 
    
    <p> The goal of Task 2 is to propose a temporal persistence classifier which can mitigate performance drop over short and long periods of time compared to a test set from the same time frame as training. </p> 
    <p> Given a test set from the same time frame as training and evaluation sets from short/long time periods, the task is to design a classifier that can mitigate short/long term performance drops. </p>
    <p> The organizers will provide a training set collected over a time frame up to a time t and two test sets: test set from time t and test set from time t+i where i=1 for sub-task A and i>1 for subtask B. </p>
    <p> <b>Sub-task short-term persistence.</b> In this task, participants will be asked to develop models that demonstrate performance persistence over short periods of time (within 1 year from the training data).</p>
    <p> <b>Sub-task long-term persistence.</b> In this task, participants will be asked to develop models that demonstrate performance persistence over a longer period of time (over 1 year apart from the training data).</p>
    
    
    <h1>Evaluation</h1>
    <h2> Task 1. LongEval-Retrieval: </h2>
    <p> The submitted systems will be evaluated in two ways: </p>
    <p>(1) <b><i>nDCG scores</i></b> calculated on a test set provided for the sub-tasks. Such a classical evaluation measure is consistent with Web search, for which the discount emphasises the ordering of the top results. </p>
    <p>(2) <b><i>Relative nDCG Drop (RnD)</i></b> measured by computing the difference between nDCG on within a time heldout test data vs short- or long-term testing sets. This measure relies on the within a time data, and supports the evaluation of the impact of the data changes on the systems’ results. </p>
    <p>These measures will be used to assess the extent to which systems provide good results, but also the extent to which they are robust against the changes within the data (queries/documents) along time. Using these evaluation measures, a system that has good results using nDCG, and also good results according to the RnD measure is considered to be able to cope with the evolution over time of the Information Retrieval collection.</p>

    <br>
    <h2> Task 2. LongEval-Classification: </h2>
    <p> The performance of the submissions will be evaluated in two ways:</p>
    <p>(1) <b><i> Macro-averaged F1-score </i></b> on the testing set of the corresponding sub-task;</p>
    <p>(2) <b><i> Relative Performance Drop (RPD) </i></b> measured by computing the difference between performance on "within-period" data vs short- or long-term distant testing sets. </p>
    <p> The submissions will be ranked based on the first metric of Macro-averaged F1. </p>
    
</body>
</html>
